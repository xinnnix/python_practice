{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f9a4a0-933b-48f6-bba2-706435fff075",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca43634-1934-4b74-be1b-f724cdd3cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_token(token):\n",
    "    vector = np.zeros((len(vocabulary),))\n",
    "    token_index = vocabulary[token]\n",
    "    vector[token_index] = 1\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a11542c-f1b7-4101-a8bc-21558f041e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "class Vectorizer:\n",
    "    \n",
    "    def standardize(self, text):\n",
    "        text = text.lower()\n",
    "        return ''.join(char for char in text\n",
    "                       if char not in string.punctuation)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = self.standardize(text)\n",
    "        return text.split()\n",
    "    \n",
    "    def make_vocabulary(self, dataset):\n",
    "        self.vocabulary = {'':0, '[UNK]':1}\n",
    "        for text in dataset:\n",
    "            text = self.standardize(text)\n",
    "            tokens = self.tokenize(text)\n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary:\n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "        self.inverse_vocabulary = dict(\n",
    "            (v, k) for k, v in self.vocabulary.items())\n",
    "        \n",
    "    def encode(self, text):\n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "    \n",
    "    def decode(self, int_sequence):\n",
    "        return ' '.join(self.inverse_vocabulary.get(i, ['UNK']) for i in int_sequence)\n",
    "    \n",
    "vectorizer = Vectorizer()\n",
    "dataset = [\n",
    "    'I write, erase, rewrite',\n",
    "    'Erase again, and then',\n",
    "    'A poppy blooms.',\n",
    "]\n",
    "vectorizer.make_vocabulary(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77533ce7-34af-4594-872a-baaabdd3ca8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 1, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'I write, rewrite, and still rewrite again'\n",
    "encoded_sentence = vectorizer.encode(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee0a054-45d1-4ce1-a77e-4f7f8f009e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70296c59-4a00-43b4-a360-96ec060a442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6c29c0-7b49-4e14-bb37-0afcb6c8c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a36c14ea-93b3-491e-8cba-089c7fd178ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization_fn(string_tensor):\n",
    "    lowercase_string = tf.strings.lower(string_tensor)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase_string, f'[{re.escape(string.punctuation)}]','')\n",
    "\n",
    "def custom_split_fn(string_tensor):\n",
    "    return tf.strings.split(string_tensor)\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode='int',\n",
    "    standardize=custom_standardization_fn,\n",
    "    split=custom_split_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44badca7-890e-40b8-bc0c-878712a993fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948fd9c8-d298-43a1-b8da-fa7a206a09ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'erase',\n",
       " 'write',\n",
       " 'then',\n",
       " 'rewrite',\n",
       " 'poppy',\n",
       " 'i',\n",
       " 'blooms',\n",
       " 'and',\n",
       " 'again',\n",
       " 'a']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66f0ec16-bf73-4b57-a65a-64ce9be49cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d57bcce9-4d2f-44c5-9464-03be438c743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'I write, rewrite, and still rewrite again'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdd393fb-cf91-4acd-bcfe-0c475b74b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "encoded_sentence = text_vectorization(test_sentence)\n",
    "print(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92daa6b1-eac2-44a2-89cf-8ae5d54725eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = dict(enumerate(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45f49d86-510f-4220-a659-88ecc3e7ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'erase',\n",
       " 3: 'write',\n",
       " 4: 'then',\n",
       " 5: 'rewrite',\n",
       " 6: 'poppy',\n",
       " 7: 'i',\n",
       " 8: 'blooms',\n",
       " 9: 'and',\n",
       " 10: 'again',\n",
       " 11: 'a'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f67c9f5c-de00-453d-8795-3cf97ca39e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i write rewrite and [UNK] rewrite again\n"
     ]
    }
   ],
   "source": [
    "decoded_sentence = ' '.join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    "print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edd0c8-c195-4bdd-83ce-a6151ac3bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sequence_dataset = string_dataset.map(\n",
    "    text_vectorization,\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "#parallelize the map() call across multiple CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884bfdf-e4d3-4d65-9a51-e3515d73dbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = keras.Input(shape=(), dtype='string')\n",
    "vectorized_text = text_vectorization(text_input)\n",
    "embedded_input = keras.layers.Embedding(...)(vectorized_text)\n",
    "output = ...\n",
    "model = keras.Model(text_input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe45ea5-ff91-4c6b-9cf5-092ccd722317",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0438a992-42c7-4bf2-b5dc-567bcf52fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5960a3f-2967-477f-a336-515c99d724a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r aclImdb/train/unsup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddb4b0d-e418-45fa-b0a5-bb316b62ce32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "!cat aclImdb/train/pos/4077_10.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf597d9e-9d48-4af9-ac0b-b24f62605086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a5031cd-8820-4709-b7d9-c64b41a314c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path('aclImdb')\n",
    "val_dir = base_dir/'val'\n",
    "train_dir = base_dir/'train'\n",
    "for category in ('neg', 'pos'):\n",
    "    os.makedirs(val_dir/category)\n",
    "    files = os.listdir(train_dir/category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2*len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir/category/fname,\n",
    "                    val_dir/category/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "346986f0-a6ed-4faa-add6-036b2e96f8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train', batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/val', batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e398a942-f105-44db-b162-165e7aedc999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (32,)\n",
      "inputs.dtype:  <dtype: 'string'>\n",
      "targets.shape:  (32,)\n",
      "targets.dtype:  <dtype: 'int32'>\n",
      "inputs[0]:  tf.Tensor(b\"LES CONVOYEURS ATTENDENT was the first film I saw in 2000 and I doubt I'll see a better one this year. This beautiful tragicomedy by Belgian filmmaker Beno\\xc3\\xaet Mariage is set in the industrial wastelands of Wallonia. Beno\\xc3\\xaet Poelvoorde plays a father who desperately wants his son to win a car (a Lada!) for him. To do this the son has to break the record opening doors. What the father actually wants his for his son to be someone, because he himself has never made it further as the reporter of local news for a newspaper ironically called L'Espoir (Hope). Of course nothing works out as planned. This film can best be compared to Aki Kaurism\\xc3\\xa4ki's DRIFTING CLOUDS, although it is more dramatic and the humour is darker. Just like in that film however the tone is more melancholic than depressing and the ending upbeat, without being unrealistically happy. The humour is absurd, without making the plot unbelievable, and Mariage finds stunning images in the bleak settings that never seem artificial. The best thing about LES CONVOYEURS ATTENDENT is the acting by Poelvoorde. This actor shot to fame with the also brilliant cult-classic C'EST ARRIV\\xc3\\x89 PR\\xc3\\x88S DE CHEZ VOUS in which he played the charismatic hitman Ben. Since then he only played two small roles in films that were not released in the Netherlands, because, as he said in an interview, he was not convinced of his own acting capabilities and all the roles he was offered were reprises of the Ben character. With his return to a leading role in LCA there should be no doubt anymore about his acting. He's simply brilliant as a man stupid and evil enough to put his family in misery, but smart enough to realize what he's done and be torn by remorse about it. A must see.\", shape=(), dtype=string)\n",
      "targets[0]:  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print('inputs.shape: ', inputs.shape)\n",
    "    print('inputs.dtype: ', inputs.dtype)\n",
    "    print('targets.shape: ', targets.shape)\n",
    "    print('targets.dtype: ', targets.dtype)\n",
    "    print('inputs[0]: ', inputs[0])\n",
    "    print('targets[0]: ', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b87e862-cb1b-4923-8038-2cfe345ded9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode='multi_hot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36818863-7167-45d8-b4f1-e7c6c03736dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only_train_ds = train_ds.map(lambda x,y : x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31365db8-6121-46f6-a6ed-e84179c7753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x,y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x,y: (text_vectorization(x),y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa29d9b7-ccd0-45b8-9132-7e8e7a1e02c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape:  (32, 20000)\n",
      "inputs.dtype:  <dtype: 'float32'>\n",
      "targets.shape:  (32,)\n",
      "targets.dtype:  <dtype: 'int32'>\n",
      "inputs[0]:  tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]:  tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print('inputs.shape: ', inputs.shape)\n",
    "    print('inputs.dtype: ', inputs.dtype)\n",
    "    print('targets.shape: ', targets.shape)\n",
    "    print('targets.dtype: ', targets.dtype)\n",
    "    print('inputs[0]: ', inputs[0])\n",
    "    print('targets[0]: ', targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f057851-2fa8-411d-a61f-09d5e49d96bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation='relu')(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1b5cbe-5b51-47c7-be28-2aa040972d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1db5b54-a478-456e-a06f-29adb00fc00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 0.4029 - accuracy: 0.8299 - val_loss: 0.2606 - val_accuracy: 0.8974\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2681 - accuracy: 0.9018 - val_loss: 0.2565 - val_accuracy: 0.9004\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.2327 - accuracy: 0.9186 - val_loss: 0.2735 - val_accuracy: 0.9002\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2291 - accuracy: 0.9240 - val_loss: 0.2811 - val_accuracy: 0.9022\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2170 - accuracy: 0.9291 - val_loss: 0.2932 - val_accuracy: 0.8986\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2153 - accuracy: 0.9301 - val_loss: 0.3022 - val_accuracy: 0.8978\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2163 - accuracy: 0.9342 - val_loss: 0.3098 - val_accuracy: 0.8952\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2094 - accuracy: 0.9352 - val_loss: 0.3207 - val_accuracy: 0.8914\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2101 - accuracy: 0.9354 - val_loss: 0.3253 - val_accuracy: 0.8960\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2033 - accuracy: 0.9362 - val_loss: 0.3302 - val_accuracy: 0.8938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc90ca47490>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_1gram.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(), validation_data=binary_1gram_val_ds, epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc66f840-addd-4497-bd2c-3e20bed04a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 4ms/step - loss: 0.2837 - accuracy: 0.8892\n",
      "Test acc: 0.889\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('binary_1gram.keras')\n",
    "print(f'Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce418a69-3899-4787-b683-28f064cdc3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='multi_hot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c1b1aba-4536-4007-86b2-261081a93112",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "968055bc-d5fb-4270-baa8-a5429bbc7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9aef009d-ff77-4fc5-8153-7391b3ec14ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 6s 9ms/step - loss: 0.3800 - accuracy: 0.8393 - val_loss: 0.2499 - val_accuracy: 0.9064\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.2470 - accuracy: 0.9119 - val_loss: 0.2511 - val_accuracy: 0.9116\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2153 - accuracy: 0.9293 - val_loss: 0.2637 - val_accuracy: 0.9100\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2015 - accuracy: 0.9384 - val_loss: 0.2884 - val_accuracy: 0.9072\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1943 - accuracy: 0.9434 - val_loss: 0.2921 - val_accuracy: 0.9096\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 5s 7ms/step - loss: 0.1905 - accuracy: 0.9445 - val_loss: 0.3028 - val_accuracy: 0.9084\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1919 - accuracy: 0.9487 - val_loss: 0.3125 - val_accuracy: 0.9074\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1837 - accuracy: 0.9501 - val_loss: 0.3244 - val_accuracy: 0.9086\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1861 - accuracy: 0.9503 - val_loss: 0.3322 - val_accuracy: 0.9050\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 7ms/step - loss: 0.1910 - accuracy: 0.9528 - val_loss: 0.3272 - val_accuracy: 0.9074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc88e2d4af0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('binary_2gram.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(), validation_data=binary_2gram_val_ds, epochs=10,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92daa25-78a7-4555-b92e-e00a33d3189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2669 - accuracy: 0.8981\n",
      "Test acc: 0.898\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('binary_2gram.keras')\n",
    "print(f'Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e963c23a-ad00-4fa0-891e-c248a0e89419",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "219cd9c5-5d37-40df-bb90-2d81025fc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode='tf_idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffb87a1c-3abc-4f63-a37d-51da11b05a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c0a5788-8e1e-4cd5-abc0-db871e019d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a77cde3-c6f4-4bcd-8bdc-9cb92014b74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 7s 10ms/step - loss: 0.5159 - accuracy: 0.7511 - val_loss: 0.2819 - val_accuracy: 0.8900\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3807 - accuracy: 0.8410 - val_loss: 0.2856 - val_accuracy: 0.8816\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3266 - accuracy: 0.8723 - val_loss: 0.2937 - val_accuracy: 0.8810\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3051 - accuracy: 0.8806 - val_loss: 0.3013 - val_accuracy: 0.8848\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2971 - accuracy: 0.8813 - val_loss: 0.3179 - val_accuracy: 0.8654\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2849 - accuracy: 0.8853 - val_loss: 0.3276 - val_accuracy: 0.8586\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2760 - accuracy: 0.8905 - val_loss: 0.3201 - val_accuracy: 0.8738\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2750 - accuracy: 0.8896 - val_loss: 0.3321 - val_accuracy: 0.8644\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2682 - accuracy: 0.8921 - val_loss: 0.3469 - val_accuracy: 0.8626\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2672 - accuracy: 0.8932 - val_loss: 0.3434 - val_accuracy: 0.8710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc8a2eff880>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('tfidf_2gram.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(tfidf_2gram_train_ds.cache(), validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7bf43de0-cf8f-47ea-9733-5630db4adafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3043 - accuracy: 0.8742\n",
      "Test acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('tfidf_2gram.keras')\n",
    "print(f'Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09273634-bb4c-44fa-90ad-b7c76729f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,),dtype='string')\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19f363bb-0c0f-423b-945f-4ccc7714058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.31 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    ['That was an excellent movie, I loved it.'],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f'{float(predictions[0]*100):.2f} percent positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b62cd376-b8ae-4b7f-abb8-dfce344bedaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=max_length)\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x,y : (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y : (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x,y : (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa63a782-733a-4d48-8984-88805fc8ca93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41305781-6c38-4386-8806-f48946a6db76",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('one_hot_bidir_lstm.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "\n",
    "# not training this because every epoch takes about two and half hours ........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8fb8487-c085-4885-a0ab-f68b7841278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c08a4ae6-51a2-44c7-a8cc-ab37201cb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,),dtype='int64')\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('embeddings_bidir_lstm.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7229c418-f747-49d4-996f-a53ac59c8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=10, output_dim=256, mask_zero=True)\n",
    "some_input = [\n",
    "    [4,3,2,1,0,0,0],\n",
    "    [5,4,3,2,1,0,0],\n",
    "    [2,1,0,0,0,0,0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "26aab55f-35db-4d08-acc5-304888119a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = embedding_layer.compute_mask(some_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c0f458e-c8fd-40bd-b26a-85b7050828b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 7), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True, False, False, False],\n",
       "       [ True,  True,  True,  True,  True, False, False],\n",
       "       [ True,  True, False, False, False, False, False]])>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a002103b-912a-4bba-8763-1c08e56366e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 583s 918ms/step - loss: 0.3935 - accuracy: 0.8209 - val_loss: 0.2786 - val_accuracy: 0.8868\n",
      "Epoch 2/10\n",
      "  9/625 [..............................] - ETA: 9:13 - loss: 0.2720 - accuracy: 0.8958"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_bidir_lstm_with_masking.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m ]\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint_train_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mint_val_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,),dtype='int64')\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('embeddings_bidir_lstm_with_masking.keras', save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa2569f-0fa2-498b-8032-1e97aa3a1591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d624f-33af-4cdd-b99b-67edf8af0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c30d268b-8183-4e7a-b718-1142874cfcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841afba-9b9f-4bb9-b40f-a3bcece2c9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_glove_file = 'glove.6b.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(f'Found {len(embeddings_index)} word vectors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794e6fc-e181-46a5-b8a8-f120ea6d5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bef9debf-f492-4ad0-a5c4-3fd32c361a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim, \n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650e92e-6bf6-45ab-9fee-1dde57a07703",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a099a-5df5-4bbc-8367-8540501879f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[\n",
    "    keras.callbacks.ModelCheckpoint('glove_embeddings_sequence_model.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7d9d9-45fc-4283-8c45-70670a8f75c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(input_sequence):\n",
    "    output = np.zeros(shape=(input_sequence.shape))\n",
    "    for i, pivot_vector in enumerate(input_sequence):\n",
    "        scores = np.zeros(shape=(len(input_sequence),))\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            scores[j] = np.dot(pivot_vector, vector.T)\n",
    "        scores /= np.sqrt(input_sequence.shape[1])\n",
    "        scores = softmax(scores)\n",
    "        new_pivot_representation = np.zeros(shape=pivot_vector.shape)\n",
    "        for j, vector in enumerate(input_sequence):\n",
    "            new_pivot_representation += vector * scores[j]\n",
    "        output[i] = new_pivot_representation\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0099f1-d10a-4eb7-a6f9-a03ed6b4c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "embed_dim = 256\n",
    "mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "outputs = mha_layer(inputs, inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fa5c86cb-3157-44b0-9de5-729fecbd406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8b336b7b-15eb-4e68-8933-20e6c645b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential([\n",
    "            layers.Dense(dense_dim, activation='relu'),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        \n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dense_dim': self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a13d802-b67e-4198-a865-2913849cdf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 256)        543776    \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 256)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,664,033\n",
      "Trainable params: 5,664,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype='int64')\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab99cc6-0025-4172-a156-8b992ecd0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint('transformer_encoder.keras', save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147886fa-5df7-4a16-a753-afd7c974eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('transformer_encoder.keras', custom_objects={'TransformerEncoder':\n",
    "                                                                             TransforerEncoder})\n",
    "print(f'Test acc: {model.evaluate(int_test_ds)[1]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37fa145b-d2e8-4dd4-b211-fa8eb1e45b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = slef.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "    \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'output_dim':self.output_dim,\n",
    "                       'sequence_length':self.sequence_length,\n",
    "                       'input_dim':self.input_dim,\n",
    "                      })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d013e-336b-4b97-bbee-0d31c3892861",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93d94952-91a0-4a82-97bc-95f446c539b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "784068b1-f992-453c-9c6c-12abbf9deae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = 'spa-eng/spa.txt'\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split('\\n')[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split('\\t')\n",
    "    spanish = '[start]' + spanish + '[end]'\n",
    "    text_pairs.append((english,spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0e823fa-f04e-4082-b2dd-bb6faad2c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Do you feel better now?', '[start]Te sents mejor ahora?[end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c035006-ea42-44f5-87e2-64df39c962df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_sameples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_sameples]\n",
    "val_pairs = text_pairs[num_train_sameples:num_train_sameples+num_val_samples]\n",
    "test_pairs = text_pairs[num_train_sameples+num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b6f0ab9-796d-41d4-b4e5-9b0640709d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b124a66f-e6f3-47d5-a668-911a4b2554fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + ''\n",
    "strip_chars = strip_chars.replace('[', '')\n",
    "strip_chars = strip_chars.replace(']', '')\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, f'[{re.escape(strip_chars)}]', '')\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length+1,\n",
    "    standardize=custom_standardization)\n",
    "\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dabbffae-2121-455b-a2b6-81658179d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({'english': eng,\n",
    "             'spanish': spa[:, :-1],\n",
    "            }, spa[:, :-1])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffe647b0-baf6-42ec-9c2e-dcfe2c12398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-14 15:38:21.721158: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bf2d5159-6103-438c-9ccc-53aba9514f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(sequence_length,), dtype='int64')\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "x = layers.LSTM(32, return_sequences=True)(x)\n",
    "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7516cc2e-2339-42fc-9161-1dbf5207b230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embed_dim = 256\n",
    "latent_dim = 1024\n",
    "\n",
    "source = keras.Input(shape=(None,), dtype='int64', name='english')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
    "encoded_source = layers.Bidirectional(\n",
    "    layers.GRU(latent_dim), merge_mode='sum')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "920967ec-0e3f-4f8d-83fb-69f2cad4cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_target = keras.Input(shape=(None,), dtype='int64', name='spanish')\n",
    "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
    "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
    "x = decoder_gru(x, initial_state=encoded_source)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "target_next_step = layers.Dense(vocab_size, activation='softmax')(x)\n",
    "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74aa1ba-1f7b-4614-9950-9432457ecb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_rnn.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)\n",
    "\n",
    "# this takes way too long to train... forget it......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814dde8-0657-4e43-ace9-e19e265dfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = '[start]'\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
    "        next_token_predictions = seq2seq_rnn.predict(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        samepled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[samepled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_token\n",
    "        if sampled_token == '[end]':\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print('-')\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9cfd9e-e5b1-4ea7-b3a4-fbb649d1f55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
