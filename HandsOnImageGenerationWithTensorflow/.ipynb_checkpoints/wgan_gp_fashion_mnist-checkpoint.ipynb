{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a461115c-0a6d-4f8c-857d-39bc4937d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc4731-30a1-4fc8-9711-e2e3868e0090",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_info = tfds.load('fashion_mnist', split='train', shuffle_files=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90a795-1433-41f4-9ce4-a4396b23bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a2b47-685a-418c-bc55-44eadb4ff6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_shape = (32,32,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b87abf-cedd-4b9f-9874-310119eb0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(features):\n",
    "    image = tf.image.resize(features['image'], image_shape[:2])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792a51a-2e36-499e-9f5e-5e341e650fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = ds_train.map(preprocess)\n",
    "ds_train = ds_train.shuffle(ds_info.splits['train'].num_examples)\n",
    "ds_train = ds_train.batch(batch_size, drop_remainder=True).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcfee28-b31c-4eb1-985a-077cbeb4399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = ds_info.splits['train'].num_examples\n",
    "train_steps_per_epoch = round(train_num/batch_size)\n",
    "print(train_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f7d8c-999f-41f2-a126-0a8f6930743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP():\n",
    "    \n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "        self.z_dim = 128\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        #critic\n",
    "        self.n_critic = 5\n",
    "        self.penalty_const = 10\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.trainalbe = False\n",
    "        \n",
    "        self.optimizer_critic = Adam(1e-4, 0.5, 0.9)\n",
    "        \n",
    "        #build generator pipeline with frozen critic\n",
    "        self.generator = self.build_generator()\n",
    "        critic_output = self.critic(self.generator.output)\n",
    "        self.model = Model(self.generator.input, critic_output)\n",
    "        self.model.compile(loss = self.wasserstein_loss,\n",
    "                           optimizer = Adam(1e-4, 0.5, 0.9))\n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        \n",
    "        w_loss = -tf.reduce_mean(y_true*y_pred)\n",
    "        \n",
    "        return w_loss\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \n",
    "        DIM = 128\n",
    "        \n",
    "        model = tf.keras.Sequential(name='Generator')\n",
    "        \n",
    "        model.add(layers.Input(shape = [self.z_dim]))\n",
    "        \n",
    "        model.add(layers.Dense(4*4*4*DIM))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        model.add(layers.Reshape((4,4,4*DIM)))\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation='bilinear'))\n",
    "        model.add(layers.Conv2D(2*DIM, 5, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation='bilinear'))\n",
    "        model.add(layers.Conv2D(DIM, 5, padding='same'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.LeakyReLU())\n",
    "        \n",
    "        model.add(layers.UpSampling2D((2,2), interpolation='bilinear'))\n",
    "        model.add(layers.Conv2D(image_shape[-1], 5, padding='same', activation='tanh'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def build_critic(self):\n",
    "        \n",
    "        DIM = 128\n",
    "        \n",
    "        model = tf.keras.Sequential(name='Critic')\n",
    "        model.add(layers.Input(shape = self.input_shape))\n",
    "        \n",
    "        model.add(layers.Conv2D(1*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "        model.add(layers.Conv2D(2*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "        model.add(layers.Conv2D(4*DIM, 5, strides=2, padding='same', use_bias=False))\n",
    "        model.add(layers.LeakyReLU(0.2))\n",
    "        \n",
    "        model.add(layers.Flatten())\n",
    "        model.add(layers.Dense(1))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def gradient_loss(self, grad):\n",
    "        \n",
    "        loss = tf.square(grad)\n",
    "        loss = tf.reduce_sum(loss, axis=np.arange(1, len(loss.shape)))\n",
    "        loss = tf.sqrt(loss)\n",
    "        loss = tf.reduce_mean(tf.square(loss-1))\n",
    "        loss = self.penalty_const * loss\n",
    "        return loss\n",
    "    \n",
    "    def train_critic(self, real_images, batch_size):\n",
    "        \n",
    "        real_labels = tf.ones(batch_size)\n",
    "        fake_labels = -tf.ones(batch_size)\n",
    "        \n",
    "        g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "        fake_images = self.generator.predict(g_input)\n",
    "        \n",
    "        with tf.GradientTape() as gradient_tape, tf.GradientTape() as total_tape:\n",
    "            \n",
    "            # forward pass\n",
    "            pred_fake = self.critic(fake_images)\n",
    "            pred_real = self.critic(real_images)\n",
    "            \n",
    "            # calculate losses\n",
    "            loss_fake = self.wasserstein_loss(fake_labels, pred_fake)\n",
    "            loss_real = self.wasserstein_loss(real_labels, pred_real)\n",
    "            \n",
    "            # gradient penalty\n",
    "            epsilon = tf.random.uniform((batch_size, 1, 1, 1))\n",
    "            interpolates = epsilon * real_images + (1-epsilon) * fake_images\n",
    "            gradient_tape.watch(interpolates)\n",
    "            \n",
    "            critic_interpolates = self.critic(interpolates)\n",
    "            gradient_interpolates = gradient_tape.gradient(critic_interpolates, [interpolates])\n",
    "            gradient_penalty = self.gradient_loss(gradient_interpolates)\n",
    "            \n",
    "            # total loss\n",
    "            total_loss = loss_fake + loss_real + gradient_penalty\n",
    "            \n",
    "            # apply gradients\n",
    "            gradients = total_tape.gradient(total_loss, self.critic.variables)\n",
    "            \n",
    "            self.optimizer_critic.apply_gradients(zip(gradients, self.critic.variables))\n",
    "            \n",
    "        return loss_fake, loss_real, gradient_penalty\n",
    "    \n",
    "    def train(self, data_generator, batch_size, steps, interval=100):\n",
    "        \n",
    "        val_g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "        real_labels = tf.ones(batch_size)\n",
    "        \n",
    "        for i in range(steps):\n",
    "            for _ in range(self.n_critic):\n",
    "                real_images = next(data_generator)\n",
    "                loss_fake, loss_real, gradient_penalty = self.train_critic(real_images, batch_size)\n",
    "                critic_loss = loss_fake + loss_real +gradient_penalty\n",
    "            \n",
    "            # train generator\n",
    "            g_input = tf.random.normal((batch_size, self.z_dim))\n",
    "            g_loss = self.model.train_on_batch(g_input, real_labels)\n",
    "            if i%interval == 0:\n",
    "                msg = \"Step {}: g_loss {:.4f} critic_loss {:.4f} critic_fake {:.4f} critic_real {:.4f} penalty {:.4f}\"\\\n",
    "                .format(i, g_loss, critic_loss, loss_fake, loss_real, gradient_penalty)\n",
    "                print(msg)\n",
    "                \n",
    "                fake_images = self.generator.predict(val_g_input)\n",
    "                self.plot_images(fake_images)\n",
    "                \n",
    "    def plot_images(self, images):\n",
    "        grid_row = 1\n",
    "        grid_col = 8\n",
    "        f, axarr = plt.subplots(grid_row, grid_col, figsize=(grid_col*2.5, grid_row*2.5))\n",
    "        for row in range(grid_row):\n",
    "            for col in range(grid_col):\n",
    "                if self.input_shape[-1]==1:\n",
    "                    axarr[col].imshow(images[col, :,:,0]*0.5+0.5, cmap='gray')\n",
    "                else:\n",
    "                    axarr[col].imshow(images[col, :,:,0]*0.5+0.5)\n",
    "                axarr[col].axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010fb28-5c3f-4e91-9a5b-e4060534c521",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN_GP(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15e2cb4-02e3-41f7-a772-0a18c78c2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.train(iter(ds_train), batch_size, 5000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8ae28-6a25-40e4-ac51-a8fb829d1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6948f4d-1443-482a-87b4-7d6028084a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.critic.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f5b724-8356-4d06-acbe-8610688c8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c77b85-7af2-48cb-b94a-3b9d71724c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
